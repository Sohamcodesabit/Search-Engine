{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sohamcodesabit/Search-Engine/blob/main/Custom_Dataset_Search_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUwE0OxezNUM",
        "outputId": "b0e3d761-55c4-47a7-e97a-0d5b84dfb294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK stopwords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK punkt tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK punkt_tab tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Simple Search Engine ---\n",
            "An inverted index has been created from the sample documents.\n",
            "A TF-IDF Vectorizer has been trained on the document set.\n",
            "Type your query below or type 'exit' to quit.\n",
            "\n",
            "Enter your search query: new\n",
            "\n",
            "--- Search Results for 'new' ---\n",
            "1. Document: doc3 (Score: 0.1513)\n",
            "   Content: Researchers have discovered a new species of deep-sea fish with bioluminescent properties.\n",
            "\n",
            "2. Document: doc1 (Score: 0.1417)\n",
            "   Content: The European Union has approved a new set of sanctions against Russia over its actions in Ukraine.\n",
            "\n",
            "3. Document: doc7 (Score: 0.1358)\n",
            "   Content: Health officials are urging the public to get vaccinated as a new flu season approaches.\n",
            "\n",
            "4. Document: doc10 (Score: 0.1338)\n",
            "   Content: Tech giant releases a new smartphone with advanced camera features and a faster processor.\n",
            "\n",
            "5. Document: doc9 (Score: 0.1288)\n",
            "   Content: A new study suggests that regular exercise can significantly improve mental health and reduce stress.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Download NLTK data (only needs to be done once) ---\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK punkt tokenizer...\")\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK punkt_tab tokenizer...\")\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# --- 2. Sample Dataset ---\n",
        "# A collection of documents (e.g., news articles, reviews)\n",
        "documents = {\n",
        "    \"doc1\": \"The European Union has approved a new set of sanctions against Russia over its actions in Ukraine.\",\n",
        "    \"doc2\": \"SpaceX successfully launched a new batch of Starlink satellites, aiming to provide global internet coverage.\",\n",
        "    \"doc3\": \"Researchers have discovered a new species of deep-sea fish with bioluminescent properties.\",\n",
        "    \"doc4\": \"The global stock market saw a significant dip this week due to rising inflation concerns and new interest rate hikes.\",\n",
        "    \"doc5\": \"A new blockbuster movie about space exploration and alien contact has received rave reviews from critics.\",\n",
        "    \"doc6\": \"The government announced new environmental policies to combat climate change, focusing on renewable energy.\",\n",
        "    \"doc7\": \"Health officials are urging the public to get vaccinated as a new flu season approaches.\",\n",
        "    \"doc8\": \"The price of oil surged after a major pipeline was disrupted, affecting global supply chains.\",\n",
        "    \"doc9\": \"A new study suggests that regular exercise can significantly improve mental health and reduce stress.\",\n",
        "    \"doc10\": \"Tech giant releases a new smartphone with advanced camera features and a faster processor.\"\n",
        "}\n",
        "\n",
        "# --- 3. Text Preprocessing ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and standardizes text for processing.\n",
        "    - Lowercases text\n",
        "    - Removes punctuation\n",
        "    - Tokenizes text (splits into words)\n",
        "    - Removes stopwords (common words like 'the', 'a', 'is')\n",
        "    - Stems words (reduces words to their root form, e.g., 'running' -> 'run')\n",
        "    \"\"\"\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Preprocess all documents in the dataset\n",
        "processed_docs = {doc_id: preprocess_text(doc_text) for doc_id, doc_text in documents.items()}\n",
        "# We need the processed text as a single string for the TfidfVectorizer\n",
        "processed_docs_str = {doc_id: ' '.join(tokens) for doc_id, tokens in processed_docs.items()}\n",
        "\n",
        "\n",
        "# --- 4. Inverted Index ---\n",
        "def create_inverted_index(processed_docs):\n",
        "    \"\"\"\n",
        "    Creates an inverted index from the processed documents.\n",
        "    The index maps each term to a list of document IDs where the term appears.\n",
        "    Example: {'new': ['doc1', 'doc2'], 'sanction': ['doc1']}\n",
        "    \"\"\"\n",
        "    inverted_index = {}\n",
        "    for doc_id, tokens in processed_docs.items():\n",
        "        for token in tokens:\n",
        "            if token not in inverted_index:\n",
        "                inverted_index[token] = []\n",
        "            if doc_id not in inverted_index[token]:\n",
        "                inverted_index[token].append(doc_id)\n",
        "    return inverted_index\n",
        "\n",
        "inverted_index = create_inverted_index(processed_docs)\n",
        "\n",
        "\n",
        "# --- 5. Vector Model and Ranking (TF-IDF) ---\n",
        "# We use Scikit-learn's TfidfVectorizer for this part.\n",
        "# It handles tokenization, counting, and TF-IDF transformation in one step.\n",
        "# For consistency, we'll fit it on our already processed documents.\n",
        "doc_list = list(processed_docs_str.values())\n",
        "doc_ids = list(processed_docs_str.keys())\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "# It will convert our text documents into a matrix of TF-IDF features.\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Learn vocabulary and idf from the documents.\n",
        "tfidf_matrix = vectorizer.fit_transform(doc_list)\n",
        "\n",
        "# Get the vocabulary (the terms the vectorizer learned)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "# --- 6. Search Function ---\n",
        "def search(query, top_n=5):\n",
        "    \"\"\"\n",
        "    Performs a search on the documents.\n",
        "    1. Preprocesses the query.\n",
        "    2. Converts the query to a TF-IDF vector.\n",
        "    3. Computes the cosine similarity between the query vector and all document vectors.\n",
        "    4. Ranks documents based on similarity and returns the top N results.\n",
        "    \"\"\"\n",
        "    # Preprocess the user's query\n",
        "    processed_query = ' '.join(preprocess_text(query))\n",
        "\n",
        "    if not processed_query.strip():\n",
        "        print(\"Your query was empty after processing. Please try a different query.\")\n",
        "        return\n",
        "\n",
        "    # Transform the query into a TF-IDF vector using the learned vocabulary\n",
        "    query_vector = vectorizer.transform([processed_query])\n",
        "\n",
        "    # Compute cosine similarity between the query vector and all document vectors\n",
        "    # This gives us a score of how relevant each document is to the query.\n",
        "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Get the indices of the top N most similar documents\n",
        "    # We use np.argsort to get the indices that would sort the array,\n",
        "    # then we take the last 'top_n' indices in reverse order.\n",
        "    # We add a small check to handle cases where the query has no matching terms.\n",
        "    if np.all(cosine_similarities == 0):\n",
        "        print(\"No relevant documents found for your query.\")\n",
        "        return\n",
        "\n",
        "    top_doc_indices = np.argsort(cosine_similarities)[-top_n:][::-1]\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(f\"\\n--- Search Results for '{query}' ---\")\n",
        "    for i, idx in enumerate(top_doc_indices):\n",
        "        # We only show results with a similarity score > 0\n",
        "        if cosine_similarities[idx] > 0:\n",
        "            doc_id = doc_ids[idx]\n",
        "            original_doc = documents[doc_id]\n",
        "            score = cosine_similarities[idx]\n",
        "            print(f\"{i+1}. Document: {doc_id} (Score: {score:.4f})\")\n",
        "            print(f\"   Content: {original_doc}\\n\")\n",
        "\n",
        "# --- 7. Main Execution Block (Command-Line Interface) ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Simple Search Engine ---\")\n",
        "    print(\"An inverted index has been created from the sample documents.\")\n",
        "    print(\"A TF-IDF Vectorizer has been trained on the document set.\")\n",
        "    print(\"Type your query below or type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nEnter your search query: \")\n",
        "        if user_query.lower() == 'exit':\n",
        "            break\n",
        "        search(user_query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}